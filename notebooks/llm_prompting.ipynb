{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "api_key_llm = os.getenv(\"NVIDIA_LLM_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_retriever():\n",
    "    try:\n",
    "        \n",
    "        base_dir = os.path.dirname(os.getcwd())\n",
    "        chroma_dir = os.path.join(base_dir, \"data\", \"chromadb\")\n",
    "        \n",
    "        if not os.path.exists(chroma_dir):\n",
    "            raise Exception(\"Indexes not found\")\n",
    "        \n",
    "        embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "        \n",
    "        vectorstoredb = Chroma(\n",
    "            collection_name=\"chroma_indexes\",\n",
    "            embedding_function=embedding_model,\n",
    "            persist_directory=chroma_dir\n",
    "        )\n",
    "        \n",
    "        retriever = vectorstoredb.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "        \n",
    "        return retriever\n",
    "    except Exception as e:\n",
    "        print(f\"Error Creating the retriever! {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x000001EEB908C8B0>, search_kwargs={'k': 3})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = create_retriever()\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "# llm = ChatNVIDIA(\n",
    "#     model=\"meta/llama-3.1-70b-instruct\",\n",
    "#     api_key=api_key_llm,\n",
    "#     temperature=0.2,\n",
    "#     top_p=0.7,\n",
    "#     max_tokens=5000,\n",
    "# )\n",
    "# llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001EEB908CF40>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001EEB908E680>, model_name='Llama-3.3-70b-Specdec', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "llm = ChatGroq(\n",
    "    model=\"Llama-3.3-70b-Specdec\",\n",
    "    temperature=0.7,\n",
    "    max_tokens=None,\n",
    "    max_retries=2,\n",
    "    timeout=None\n",
    ")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBranch(branches=[(RunnableLambda(lambda x: not x.get('chat_history', False)), RunnableLambda(lambda x: x['input'])\n",
       "| VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x000001EEB908C8B0>, search_kwargs={'k': 3}))], default=ChatPromptTemplate(input_variables=['chat_history', 'input'], input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x000001EEB5786EF0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='\\n    Given a chat history and the latest user question\\n    which might refer context in the chat history,\\n    formulate a standalone question which can be understood\\n    without the chat history. Do NOT answer the question,\\n    just reformulate it if needed otherwise return it as it is.\\n'), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001EEB908CF40>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001EEB908E680>, model_name='Llama-3.3-70b-Specdec', model_kwargs={}, groq_api_key=SecretStr('**********'))\n",
       "| StrOutputParser()\n",
       "| VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x000001EEB908C8B0>, search_kwargs={'k': 3})), kwargs={}, config={'run_name': 'chat_retriever_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import create_retrieval_chain, create_history_aware_retriever\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "contextualize_q_system_prompt = \"\"\"\n",
    "    Given a chat history and the latest user question\n",
    "    which might refer context in the chat history,\n",
    "    formulate a standalone question which can be understood\n",
    "    without the chat history. Do NOT answer the question,\n",
    "    just reformulate it if needed otherwise return it as it is.\n",
    "\"\"\"\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system', contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        ('user', '{input}')\n",
    "    ]\n",
    ")\n",
    "\n",
    "history_aware_retriever = create_history_aware_retriever(llm, retriever, contextualize_q_prompt)\n",
    "history_aware_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableBranch(branches=[(RunnableLambda(lambda x: not x.get('chat_history', False)), RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x000001EEB908C8B0>, search_kwargs={'k': 3}))], default=ChatPromptTemplate(input_variables=['chat_history', 'input'], input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x000001EEB5786EF0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='\\n    Given a chat history and the latest user question\\n    which might refer context in the chat history,\\n    formulate a standalone question which can be understood\\n    without the chat history. Do NOT answer the question,\\n    just reformulate it if needed otherwise return it as it is.\\n'), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "           | ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001EEB908CF40>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001EEB908E680>, model_name='Llama-3.3-70b-Specdec', model_kwargs={}, groq_api_key=SecretStr('**********'))\n",
       "           | StrOutputParser()\n",
       "           | VectorStoreRetriever(tags=['Chroma', 'OpenAIEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x000001EEB908C8B0>, search_kwargs={'k': 3})), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | ChatPromptTemplate(input_variables=['chat_history', 'context', 'input'], input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x000001EEB5786EF0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template=\"\\n    You are a powerful assistant for question-answering tasks.\\n    Use the following pieces of retrieved context to answer\\n    the question in full detail. If you don't know the answer, say that you\\n    don't know, but do not ask the user for more information. Use only the content \\n    from the uploaded documents to answer this question. Provide a detailed\\n    explanation and a comprehensive response unless explicitly stated otherwise.\\n    \\n\\n\\n    {context}\\n\"), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])\n",
       "            | ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001EEB908CF40>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001EEB908E680>, model_name='Llama-3.3-70b-Specdec', model_kwargs={}, groq_api_key=SecretStr('**********'))\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_prompt = \"\"\"\n",
    "    You are a powerful assistant for question-answering tasks.\n",
    "    Use the following pieces of retrieved context to answer\n",
    "    the question in full detail. If you don't know the answer, say that you\n",
    "    don't know, but do not ask the user for more information. Use only the content \n",
    "    from the uploaded documents to answer this question. Provide a detailed\n",
    "    explanation and a comprehensive response unless explicitly stated otherwise.\n",
    "    \\n\\n\n",
    "    {context}\n",
    "\"\"\"\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system', system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        ('user', '{input}')\n",
    "    ]\n",
    ")\n",
    "\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 3 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is sqa?',\n",
       " 'chat_history': [],\n",
       " 'context': [Document(id='65689b25-2a99-4ab0-a2f4-aa4671da33d0', metadata={'page': 0, 'source': 'C:\\\\Users\\\\Paras Dhoon\\\\Desktop\\\\Placement 2025\\\\RAG Document Q&A\\\\data\\\\raw\\\\SQA Vs Software Testing.pdf'}, page_content='DIFFERENCE BETWEEN SOFTWARE  \\nQUALITY ASSURANCE  \\n&  \\nSOFTWARE TESTING\\nDr Lokesh Sharma')],\n",
       " 'answer': 'Software Quality Assurance (SQA) refers to the systematic process of evaluating and improving the quality of software products and processes. It involves a set of activities, methods, and techniques aimed at ensuring that software development and maintenance processes are carried out in a way that meets the required standards, regulations, and customer expectations.\\n\\nThe primary goal of SQA is to provide confidence that the software product or system meets the specified requirements, is reliable, stable, and performs as expected. SQA involves a proactive approach to identifying and mitigating potential quality risks throughout the software development life cycle, from planning and design to testing, deployment, and maintenance.\\n\\nSome key aspects of SQA include:\\n\\n1. **Quality Planning**: Defining quality objectives, policies, and procedures.\\n2. **Quality Control**: Monitoring and controlling the software development process to ensure compliance with quality standards.\\n3. **Quality Assurance**: Evaluating the software development process to ensure that it is capable of producing high-quality products.\\n4. **Quality Improvement**: Identifying areas for improvement and implementing changes to the software development process.\\n\\nSQA activities may include:\\n\\n* Developing and maintaining quality policies, procedures, and standards\\n* Conducting audits and reviews to ensure compliance with quality standards\\n* Identifying and mitigating quality risks\\n* Implementing defect prevention and detection techniques\\n* Monitoring and analyzing quality metrics\\n* Providing training and awareness programs for software development teams\\n\\nIn summary, SQA is a systematic approach to ensuring the quality of software products and processes, with the goal of providing confidence that the software meets the required standards and customer expectations.'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_question = \"What is sqa?\"\n",
    "\n",
    "response = rag_chain.invoke({\n",
    "    \"input\": user_question,\n",
    "    \"chat_history\": chat_history\n",
    "})\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='2 + 1 = 3', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 37, 'total_tokens': 45, 'completion_time': 0.002648286, 'prompt_time': 0.004367593, 'queue_time': 0.053328486999999994, 'total_time': 0.007015879}, 'model_name': 'Llama-3.3-70b-Specdec', 'system_fingerprint': 'fp_74379b522c', 'finish_reason': 'stop', 'logprobs': None}, id='run-d499017b-6c7a-4a69-aae5-ff902c957aef-0', usage_metadata={'input_tokens': 37, 'output_tokens': 8, 'total_tokens': 45})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"2+1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 3 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(id='65689b25-2a99-4ab0-a2f4-aa4671da33d0', metadata={'page': 0, 'source': 'C:\\\\Users\\\\Paras Dhoon\\\\Desktop\\\\Placement 2025\\\\RAG Document Q&A\\\\data\\\\raw\\\\SQA Vs Software Testing.pdf'}, page_content='DIFFERENCE BETWEEN SOFTWARE  \\nQUALITY ASSURANCE  \\n&  \\nSOFTWARE TESTING\\nDr Lokesh Sharma')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"Gradient Ascent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max concurrency: 20\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "print(\"Max concurrency:\", multiprocessing.cpu_count())  # Number of CPU cores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
